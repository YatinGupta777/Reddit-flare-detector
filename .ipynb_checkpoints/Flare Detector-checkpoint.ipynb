{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer    \n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import numpy as np\n",
    "set(stopwords.words('english'))\n",
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading lib\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Id to scrape\n",
    "my_client_id = \"tlaYd7tsDOqvlQ\"\n",
    "my_client_secret = \"xdRqwLkA07r8ScyJZTMsYUndrSA\"\n",
    "my_user_agent = \"scrapping r/india\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to print full panda data frame to analyze data\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_colwidth', 950)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API to access reddit data\n",
    "reddit = praw.Reddit(client_id=my_client_id, client_secret=my_client_secret, user_agent=my_user_agent)\n",
    "india_subreddit = reddit.subreddit('India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "<ul>\n",
    "    <li>Getting TOP, HOT and NEW posts of reddit using the praw API </li>\n",
    "    <li>Many JSON tags are retrived which may or may not be used in future </li>\n",
    "    <li>https://github.com/reddit-archive/reddit/wiki/JSON used to study the tags </li>\n",
    "    <li>This is done to get all the types of posts and not be trained on only one type. Also increases training data</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = []\n",
    "for post in india_subreddit.top(limit=1000):\n",
    "    top_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "\n",
    "hot_posts = []\n",
    "for post in india_subreddit.hot(limit=1000):\n",
    "    hot_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "\n",
    "\n",
    "new_posts = []\n",
    "for post in india_subreddit.new(limit=1000):\n",
    "    new_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Converting the data obtained into data frames for analyzing and combining the three types of data with duplicates removed</li>\n",
    "    <li>Adding a text column that is title + body</li>\n",
    "    <li>Removing NONE instances</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = pd.DataFrame(top_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "hot_posts = pd.DataFrame(hot_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "new_posts = pd.DataFrame(new_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "\n",
    "frames = [top_posts, hot_posts, new_posts]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we analyze the division of count of posts in respective flares\n",
    "This will tell how many and which flares are prominent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "flare_class = data[\"flare\"].value_counts()\n",
    "flare_class.plot(kind= 'bar')\n",
    "plt.title('Count of different flares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the word cloud of the four main Flares - Coronavirus, non-political, politics, askindia\n",
    "This is done to visualize which words are more used wrt flares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "Coronavirus = data[data[\"flare\"] == \"Coronavirus\"]\n",
    "Non_Political = data[data[\"flare\"] == \"Non-Political\"]\n",
    "Politics = data[data[\"flare\"] == \"Politics\"]\n",
    "AskIndia = data[data[\"flare\"] == \"AskIndia\"]\n",
    "\n",
    "Coronavirus_words = ''\n",
    "Non_Political_words = ''\n",
    "Politics_words = ''\n",
    "AskIndia_words = ''\n",
    "\n",
    "#Extracting words wrt flares\n",
    "\n",
    "for t in Coronavirus.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Coronavirus_words = Coronavirus_words + w + ' '\n",
    "        \n",
    "for t in Non_Political.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Non_Political_words = Non_Political_words + w + ' '\n",
    "        \n",
    "for t in Politics.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Politics_words = Politics_words + w + ' '\n",
    "        \n",
    "for t in AskIndia.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        AskIndia_words = AskIndia_words + w + ' '        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coronavirus Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Coronavirus_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-politcal Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Non_Political_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Politics words Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Politics_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask India words, Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(AskIndia_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of length of posts and number of words used in different flare posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coronavirus[\"post_length\"] = Coronavirus.text.apply(lambda x: len(x))\n",
    "Non_Political[\"post_length\"] = Non_Political.text.apply(lambda x: len(x))\n",
    "\n",
    "Coronavirus['number_words'] = Coronavirus.text.apply(lambda x: len(x.split()))\n",
    "Non_Political['number_words'] = Non_Political.text.apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Mean Length of Coronavirus Posts \" + str(Coronavirus[\"post_length\"].mean()))\n",
    "print(\"Mean Length of Non Political Posts \"  + str(Non_Political[\"post_length\"].mean()))\n",
    "print(\"Mean word usage of Coronavirus Posts \"  + str(Coronavirus['number_words'].mean()))\n",
    "print(\"Mean word usage of Non Political Posts \"  + str(Non_Political['number_words'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Flare Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function to build confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "<ul>\n",
    "    <li> The required data here is extracted from data collected earlier </li>\n",
    "    <li> The TOP, HOT and NEW posts are concatenated in frame </li>\n",
    "    <li> duplicates are removed </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text_flare_data = top_posts[['id','title','body','flare']]\n",
    "hot_text_flare_data = hot_posts[['id','title','body','flare']]\n",
    "new_text_flare_data = new_posts[['id','title','body','flare']]\n",
    "\n",
    "frames = [top_text_flare_data, hot_text_flare_data, new_text_flare_data]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I\n",
    "Here around 15 flares are considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the number of flare was large (40), all the flares with less than 3 posts are deleted here.\n",
    "<ul>\n",
    "<li>This is done since having large number of flares reduced the accuracy if test dataset on splitting only had the lower ones.</li>\n",
    "<li>Also model is able to predict only the most prominent ones due to less data. </li>\n",
    "<li>Creating a text column which concatenates the title and body text </li>\n",
    "<li> ALL the rows with NONE data are removed </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('flare').filter(lambda x : len(x)>3)\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "<ul>\n",
    "    <li>Creating regex emoji pattern to remove emojis</li>\n",
    "    <li> All strings are lowered </li>\n",
    "    <li> Punctuation, whitespaces,links are removed</li>\n",
    "    <li> Stop words are removed and words are stemmed</li>\n",
    "    <li> Finally it contains list of strings where each word is stemmed and cleaned</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['text'] = (data['text'].str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "empty = ['removed','deleted','nan']\n",
    "\n",
    "for i in data['text']:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words and not in empty:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "data['text'] = res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['text'], data['flare'],test_size=0.3)\n",
    "\n",
    "# X = the entire text data , Y = entire target or flares\n",
    "X = data['text']\n",
    "Y = data['flare']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# label encode the target variable \n",
    "Y = encoder.fit_transform(Y)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the words to vectors using Tfidf to be used for ML algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
    "X = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy of different ML algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_naive = accuracy_score(predictions_NB, test_y)*100\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_SVM = accuracy_score(predictions_SVM, test_y)*100\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "random_forest_classfier = ensemble.RandomForestClassifier()\n",
    "random_forest_classfier.fit(Train_X_Tfidf,train_y)\n",
    "predictions_RF = random_forest_classfier.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_rfc = accuracy_score(predictions_RF, test_y)*100\n",
    "print(\"RF Accuracy Score -> \",accuracy_score(predictions_RF, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGDClassifier \n",
    "linear classifier optimized by the SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "sgd.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_sgd = sgd.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_sgd = accuracy_score(predictions_sgd, test_y)*100\n",
    "print(\"SGD Accuracy Score -> \",accuracy_score(predictions_sgd, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logistic_Regression = LogisticRegression(n_jobs=1, C=1e5,max_iter=2000)\n",
    "Logistic_Regression.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_Logistic_Regression = Logistic_Regression.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_lr = accuracy_score(predictions_Logistic_Regression, test_y)*100\n",
    "print(\"Logistic_Regression Accuracy Score -> \",accuracy_score(predictions_Logistic_Regression, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = [accuracy_naive,accuracy_SVM,accuracy_rfc,accuracy_sgd,accuracy_lr]\n",
    "model_names = [\"Naive\",\"SVM\",\"random_forest_classfier\",\"sgd\",\"Logistic_Regression\"]\n",
    "for i in range(0,len(models)):\n",
    "    print(\"Accuracy by \" + model_names[i] + \" = \" + str(model_prediction[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on Manual Examples\n",
    "below example list is created on which the models will predict the flare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = np.array([\"Stuck in quarantine. Playing games all day\",\"This. virus is killing me\",\"Government of India needs to take action \",\"All the political ministers need to work for india\"])\n",
    "Example = pd.Series(example_list)\n",
    "Example = (Example.str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in Example:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "Example = res\n",
    "Example_Tfidf = Tfidf_vect.transform(Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Naive,SVM,random_forest_classfier,sgd,Logistic_Regression]\n",
    "for i in range(0,len(models)):\n",
    "    model = models[i]\n",
    "    predict_example = model.predict(Example_Tfidf)\n",
    "    print(\"Predictions by \" + model_names[i])\n",
    "    print(encoder.inverse_transform(predict_example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a confusion matrix to analyze the results from the best classifier yet (sgd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_y, predictions_sgd)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "Since most of the cm is 0 or very less values here the dataset is reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to get accuracy from all the ML models which were defined as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(Train_X_Tfidf,train_y,Test_X_Tfidf,test_y):\n",
    "    from sklearn import naive_bayes\n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_naive = accuracy_score(predictions_NB, test_y)*100\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y)*100)\n",
    "\n",
    "    from sklearn import svm\n",
    "    # fit the training dataset on the classifier\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_SVM = accuracy_score(predictions_SVM, test_y)*100\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)\n",
    "\n",
    "    from sklearn import ensemble\n",
    "    random_forest_classfier = ensemble.RandomForestClassifier()\n",
    "    random_forest_classfier.fit(Train_X_Tfidf,train_y)\n",
    "    predictions_RF = random_forest_classfier.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_rfc = accuracy_score(predictions_RF, test_y)*100\n",
    "    print(\"RF Accuracy Score -> \",accuracy_score(predictions_RF, test_y)*100)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    Logistic_Regression = LogisticRegression(n_jobs=1, C=1e5,max_iter=2000)\n",
    "    Logistic_Regression.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_Logistic_Regression = Logistic_Regression.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_lr = accuracy_score(predictions_Logistic_Regression, test_y)*100\n",
    "    print(\"Logistic_Regression Accuracy Score -> \",accuracy_score(predictions_Logistic_Regression, test_y)*100)\n",
    "\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    sgd.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_sgd = sgd.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_sgd = accuracy_score(predictions_sgd, test_y)*100\n",
    "    print(\"SGD Accuracy Score -> \",accuracy_score(predictions_sgd, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is made again by using the scraped data, but here only flares with greater than 100 posts are used\n",
    "<ul>\n",
    "    <li>Data is cleaned and prepared</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text_flare_data = top_posts[['id','title','body','flare']]\n",
    "hot_text_flare_data = hot_posts[['id','title','body','flare']]\n",
    "new_text_flare_data = new_posts[['id','title','body','flare']]\n",
    "\n",
    "frames = [top_text_flare_data, hot_text_flare_data, new_text_flare_data]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data = data.groupby('flare').filter(lambda x : len(x)>100)\n",
    "\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['text'] = (data['text'].str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "empty = ['removed','deleted','nan']\n",
    "\n",
    "for i in data['text']:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words and not in empty:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "data['text'] = res\n",
    "data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data obtained above is divided into train,test and vectorized /encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['text'], data['flare'],test_size=0.3)\n",
    "\n",
    "# X = the entire text data , Y = entire target or flares\n",
    "X = data['text']\n",
    "Y = data['flare']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# label encode the target variable \n",
    "Y = encoder.fit_transform(Y)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n",
    "Tfidf_vect = TfidfVectorizer(max_features=15000)\n",
    "Tfidf_vect.fit(data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
    "X = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy is measured from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(Train_X_Tfidf,train_y,Test_X_Tfidf,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE('minority')\n",
    "X_oversampled, Y_oversampled = smote.fit_resample(X, Y)\n",
    "Train_X_Tfidf_oversampled, Test_X_Tfidf_oversampled, train_y_oversampled, test_y_oversampled = model_selection.train_test_split(X_oversampled, Y_oversampled,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy is measured from the oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(Train_X_Tfidf_oversampled,train_y_oversampled,Test_X_Tfidf_oversampled,test_y_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "model.fit(Train_X_Tfidf_oversampled,train_y_oversampled)\n",
    "predictions_sgd = model.predict(Test_X_Tfidf_oversampled)\n",
    "cm = confusion_matrix(test_y_oversampled, predictions_sgd)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix indicates better values than previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III\n",
    "Increasing the amount of data by a large count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the Pushshift data, reddit r/india is scrapped for 2 years data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the helper function for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "def getPushshiftData(after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "   # print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here data is repeadtly scrapped in counts of 1000 posts from Jan 1 2018 to 10th April 2020.\n",
    "### Calls to reddit API after every 1000 posts as that is the limit\n",
    "#### This function takes a approx 10-20 minutes\n",
    "#### Pandas dataframe is downloaded once the download finishes. Hence this code is commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subData = []\n",
    "# subCount = 0\n",
    "# #Subreddit to query\n",
    "# sub='india'\n",
    "# #before and after dates\n",
    "\n",
    "# #unix time is provided\n",
    "# after = \"1514764800\"  #January 1st 2018\n",
    "# before = \"1586476800\" #10th April 2020\n",
    "\n",
    "# data = getPushshiftData(after, before, sub)\n",
    "\n",
    "# while len(data) > 0:\n",
    "#     for submission in data:\n",
    "#         try:\n",
    "#             flare = submission['link_flair_text']\n",
    "#         except KeyError:\n",
    "#             flare = None\n",
    "         \n",
    "#         if flare == None:\n",
    "#             continue\n",
    "            \n",
    "#         sub_id = submission['id']\n",
    "#         title = submission['title']\n",
    "#         try:\n",
    "#             selftext = submission['selftext']\n",
    "#         except KeyError:\n",
    "#             selftext = None\n",
    "        \n",
    "#         subData.append([sub_id,title,selftext,flare])    \n",
    "#         subCount+=1\n",
    "#     print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])),end = \" \")\n",
    "#     #after every 1000 posts,after time is changed for API by using the last post extracted\n",
    "#     after = data[-1]['created_utc']\n",
    "#     data = getPushshiftData(after, before, sub)\n",
    "              \n",
    "# original_expanded_data = pd.DataFrame(subData,columns=['id', 'title', 'body', 'flare'])\n",
    "# original_expanded_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# original_expanded_data.to_pickle(\"original_expanded_data.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the new expanded data flare count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                                                  60969\n",
       "Non-Political                                             59573\n",
       "AskIndia                                                  33075\n",
       "Business/Finance                                          13181\n",
       "Science/Technology                                        10257\n",
       "                                                          ...  \n",
       "Not Related to India                                          1\n",
       "| Witch-hunting/Targeting User |                              1\n",
       "Low-effort Post.                                              1\n",
       "Not in English. Removed                                       1\n",
       "Personal Info. Unverified Content. TIL Rule Violation.        1\n",
       "Name: flare, Length: 222, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_data = pd.read_pickle(\"original_expanded_data.pkl\")\n",
    "expanded_data.drop_duplicates(keep='last',inplace=True)\n",
    "expanded_data['text'] = expanded_data['title'].str.cat(expanded_data['body'], sep =\" \")\n",
    "expanded_data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the flares which have 3000 posts +\n",
    "### seeing the count now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_data = expanded_data.groupby('flare').filter(lambda x : len(x)>3000)\n",
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "flare_class = expanded_data[\"flare\"].value_counts()\n",
    "flare_class.plot(kind= 'bar')\n",
    "plt.title('Count of different flares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flare</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>210245</td>\n",
       "      <td>200593</td>\n",
       "      <td>34841</td>\n",
       "      <td>11</td>\n",
       "      <td>199121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>dqy8o1</td>\n",
       "      <td>Slag Crusher Plant Manufacturer Exporters in I...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>153490</td>\n",
       "      <td>60904</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title    body  \\\n",
       "count   210245                                             210245  210245   \n",
       "unique  210245                                             200593   34841   \n",
       "top     dqy8o1  Slag Crusher Plant Manufacturer Exporters in I...           \n",
       "freq         1                                                131  153490   \n",
       "\n",
       "           flare    text  \n",
       "count     210245  210245  \n",
       "unique        11  199121  \n",
       "top     Politics          \n",
       "freq       60904     242  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "expanded_data['text'] = (expanded_data['text'].str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "expanded_data = expanded_data.mask(expanded_data.eq('None')).dropna()\n",
    "res = []\n",
    "empty = ['removed','deleted','nan']\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "for i in expanded_data['text']:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words and not in empty:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "expanded_data['text'] = res\n",
    "expanded_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  first draft nrc assam releas check name \n",
       "1                 best fastest way doubl 1 lakh rupe legal \n",
       "2                    anyon know what go back truck comment \n",
       "3         like jadhav couldnt hug mother glass screen ya...\n",
       "4         2018              \n",
       "                                ...                        \n",
       "224488           done homeless beggar india lockdown remov \n",
       "224489           swabhimaan top rate 90 serial doordarshan \n",
       "224490    latest indian hindi full movi hd horror myster...\n",
       "224491    coronaviru indian doctor warn prime minist ppe...\n",
       "224492        coronaviru centr concern polaris religi line \n",
       "Name: text, Length: 210245, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding/vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(expanded_data['text'], expanded_data['flare'],test_size=0.3)\n",
    "\n",
    "# X = the entire text data , Y = entire target or flares\n",
    "X = expanded_data['text']\n",
    "Y = expanded_data['flare']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# label encode the target variable \n",
    "Y = encoder.fit_transform(Y)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n",
    "Tfidf_vect = TfidfVectorizer(max_features=30000)\n",
    "Tfidf_vect.fit(expanded_data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
    "X = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling the Data since some flares have much increased posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({7: 60904, 3: 59430, 0: 32861, 1: 13169, 8: 10245, 6: 9858, 2: 6563, 10: 5522, 5: 4545, 9: 4117, 4: 3031})\n",
      "Counter({0: 3031, 1: 3031, 2: 3031, 3: 3031, 4: 3031, 5: 3031, 6: 3031, 7: 3031, 8: 3031, 9: 3031, 10: 3031})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "print('Original dataset shape %s' % Counter(Y))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_undersampled, Y_undersampled = rus.fit_resample(X, Y)\n",
    "Train_X_Tfidf_undersampled, Test_X_Tfidf_undersampled, train_y_undersampled, test_y_undersampled = model_selection.train_test_split(X_undersampled, Y_undersampled,test_size=0.3)\n",
    "print(Counter(Y_undersampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  55.74327701689493\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf_undersampled,train_y_undersampled)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf_undersampled)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_naive = accuracy_score(predictions_NB, test_y_undersampled)*100\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y_undersampled)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the accuracy using this data and models defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score ->  52.70418874337699\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "random_forest_classfier = ensemble.RandomForestClassifier()\n",
    "random_forest_classfier.fit(Train_X_Tfidf_undersampled,train_y_undersampled)\n",
    "predictions_RF = random_forest_classfier.predict(Test_X_Tfidf_undersampled)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_rfc = accuracy_score(predictions_RF, test_y_undersampled)*100\n",
    "print(\"RF Accuracy Score -> \",accuracy_score(predictions_RF, test_y_undersampled)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Accuracy Score ->  54.16375087473758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "sgd.fit(Train_X_Tfidf_undersampled,train_y_undersampled)\n",
    "# predict the labels on validation dataset\n",
    "predictions_sgd = sgd.predict(Test_X_Tfidf_undersampled)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_sgd = accuracy_score(predictions_sgd, test_y_undersampled)*100\n",
    "print(\"SGD Accuracy Score -> \",accuracy_score(predictions_sgd, test_y_undersampled)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression Accuracy Score ->  49.75901322256397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yatingupta/Github/Reddit-flare-detector/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# Logistic_Regression = LogisticRegression(n_jobs=1, C=1e5,max_iter=20000)\n",
    "# Logistic_Regression.fit(Train_X_Tfidf,train_y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_Logistic_Regression = Logistic_Regression.predict(Test_X_Tfidf)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# accuracy_lr = accuracy_score(predictions_Logistic_Regression, test_y)*100\n",
    "# print(\"Logistic_Regression Accuracy Score -> \",accuracy_score(predictions_Logistic_Regression, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "# SVM.fit(Train_X_Tfidf,train_y)\n",
    "# # predict the labels on validation dataset\n",
    "# predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# accuracy_SVM = accuracy_score(predictions_SVM, test_y)*100\n",
    "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Naive\n",
    "predict_example = model.predict(Example_Tfidf)\n",
    "print(encoder.inverse_transform(predict_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below code will download the model, tfidf vectorizer and encoder to be directly used in Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = Naive\n",
    "pickle.dump(model, open(\"model.pkl\",\"wb\"))\n",
    "pickle.dump(Tfidf_vect, open(\"tfidf.pickle\", \"wb\"))\n",
    "np.save('classes.npy', encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is used to test the endpoint by sending request from test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with open('test.txt', 'rb') as f:\n",
    "    r = requests.post('https://reddit-flare-detector.herokuapp.com/automated_testing', files={'test.txt': f})\n",
    "    a = r.json()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
