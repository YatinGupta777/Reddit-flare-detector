{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer    \n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "#nltk.download('stopwords')\n",
    "set(stopwords.words('english'))\n",
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Id to scrape\n",
    "my_client_id = \"tlaYd7tsDOqvlQ\"\n",
    "my_client_secret = \"xdRqwLkA07r8ScyJZTMsYUndrSA\"\n",
    "my_user_agent = \"scrapping r/india\"\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to print full panda data frame to analyze data\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_colwidth', 950)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API to access reddit data\n",
    "reddit = praw.Reddit(client_id=my_client_id, client_secret=my_client_secret, user_agent=my_user_agent)\n",
    "india_subreddit = reddit.subreddit('India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "1. Getting TOP, HOT and NEW posts of reddit using the praw API \n",
    "2. Many JSON tags are retrived which may or may not be used in future \n",
    "3. https://github.com/reddit-archive/reddit/wiki/JSON used to study the tags \n",
    "4. This is done to get all the types of posts and not be trained on only one type. Also increases training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = []\n",
    "for post in india_subreddit.top(limit=1000):\n",
    "    top_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "\n",
    "hot_posts = []\n",
    "for post in india_subreddit.hot(limit=1000):\n",
    "    hot_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "\n",
    "new_posts = []\n",
    "for post in india_subreddit.new(limit=1000):\n",
    "    new_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Converting the data obtained into data frames for analyzing and combining the three types of data with duplicates removed\n",
    "2. Adding a text column that is title + body\n",
    "3. Removing NONE instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = pd.DataFrame(top_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "hot_posts = pd.DataFrame(hot_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "new_posts = pd.DataFrame(new_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "\n",
    "frames = [top_posts, hot_posts, new_posts]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we analyze the division of count of posts in respective flares\n",
    "This will tell how many and which flares are prominent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "flare_class = data[\"flare\"].value_counts()\n",
    "flare_class.plot(kind= 'bar')\n",
    "plt.title('Count of different flares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the word cloud of the four main Flares - Coronavirus, non-political, politics, askindia\n",
    "This is done to visualize which words are more used wrt flares and thus studying what dataset contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "Coronavirus = data[data[\"flare\"] == \"Coronavirus\"]\n",
    "Non_Political = data[data[\"flare\"] == \"Non-Political\"]\n",
    "Politics = data[data[\"flare\"] == \"Politics\"]\n",
    "AskIndia = data[data[\"flare\"] == \"AskIndia\"]\n",
    "\n",
    "Coronavirus_words = ''\n",
    "Non_Political_words = ''\n",
    "Politics_words = ''\n",
    "AskIndia_words = ''\n",
    "\n",
    "#Extracting words wrt flares\n",
    "\n",
    "for t in Coronavirus.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Coronavirus_words = Coronavirus_words + w + ' '\n",
    "        \n",
    "for t in Non_Political.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Non_Political_words = Non_Political_words + w + ' '\n",
    "        \n",
    "for t in Politics.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Politics_words = Politics_words + w + ' '\n",
    "        \n",
    "for t in AskIndia.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        AskIndia_words = AskIndia_words + w + ' '        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coronavirus Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Coronavirus_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-politcal Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Non_Political_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Politics words Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Politics_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask India words, Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(AskIndia_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of length of posts and number of words used in different flare posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coronavirus[\"post_length\"] = Coronavirus.text.apply(lambda x: len(x))\n",
    "Non_Political[\"post_length\"] = Non_Political.text.apply(lambda x: len(x))\n",
    "\n",
    "Coronavirus['number_words'] = Coronavirus.text.apply(lambda x: len(x.split()))\n",
    "Non_Political['number_words'] = Non_Political.text.apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Mean Length of Coronavirus Posts \" + str(Coronavirus[\"post_length\"].mean()))\n",
    "print(\"Mean Length of Non Political Posts \"  + str(Non_Political[\"post_length\"].mean()))\n",
    "print(\"Mean word usage of Coronavirus Posts \"  + str(Coronavirus['number_words'].mean()))\n",
    "print(\"Mean word usage of Non Political Posts \"  + str(Non_Political['number_words'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Flare Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function to build confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions\n",
    "1. Preprocessing Textual data\n",
    "2. Encodes Target Variable\n",
    "3. Converts into TFIDF input texts\n",
    "4. Naive Bayes\n",
    "5. SVM\n",
    "6. Random Forest\n",
    "7. SGD\n",
    "8. Logistic Regression   \n",
    "9. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def preprocessing(data):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    data = (data.str.lower() #lowercase\n",
    "                               .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                               .str.replace(emoji_pattern, '') #rem emoji\n",
    "                               .str.replace(r'http\\S+','') #rem links\n",
    "                               .str.strip() #rem trailing whitespaces\n",
    "                               .str.split()) #split by whitespaces\n",
    "\n",
    "    res = []\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    empty = ['removed','deleted','nan']\n",
    "    \n",
    "    #Removing stop words, empty words and stemming the words\n",
    "    for i in data:\n",
    "        t = \"\"\n",
    "        for j in i:\n",
    "            if j not in stop_words and j not in empty:\n",
    "                w = ps.stem(j)\n",
    "                t += w\n",
    "                t += \" \"\n",
    "        res.append(t)\n",
    "    data = res\n",
    "    return data\n",
    "\n",
    "def encode(Y,train_y,test_y):\n",
    "    encoder = LabelEncoder()\n",
    "    Y = encoder.fit_transform(Y)\n",
    "    train_y = encoder.transform(train_y)\n",
    "    test_y = encoder.transform(test_y)\n",
    "    return Y,train_y,test_y, encoder\n",
    "\n",
    "def tfidf(X,train_x,test_x):\n",
    "    Tfidf_vect = TfidfVectorizer()\n",
    "    Tfidf_vect.fit(X)\n",
    "    X = Tfidf_vect.transform(X)\n",
    "    train_x = Tfidf_vect.transform(train_x)\n",
    "    test_x = Tfidf_vect.transform(test_x)\n",
    "    return X, train_x, test_x, Tfidf_vect\n",
    "\n",
    "def Naive_Bayes_Classifier(train_x,train_y,test_x,test_y):\n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = MultinomialNB()\n",
    "    Naive.fit(train_x, train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(test_x)\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y)*100)\n",
    "    \n",
    "    return Naive\n",
    "\n",
    "def SVM_Classifier(train_x,train_y,test_x,test_y):\n",
    "    # fit the training dataset on the classifier\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(train_x,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(test_x)\n",
    "    \n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)\n",
    "    return SVM\n",
    "\n",
    "def Random_Forest_Classifier(train_x,train_y,test_x,test_y):\n",
    "    random_forest_classfier = ensemble.RandomForestClassifier()\n",
    "    random_forest_classfier.fit(train_x,train_y)\n",
    "    predictions_RF = random_forest_classfier.predict(test_x)\n",
    "    \n",
    "    print(\"RF Accuracy Score -> \",accuracy_score(predictions_RF, test_y)*100)\n",
    "    return random_forest_classfier\n",
    "\n",
    "def SGD_Classifier(train_x,train_y,test_x,test_y):\n",
    "    sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    sgd.fit(train_x,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_sgd = sgd.predict(test_x)\n",
    "    print(\"SGD Accuracy Score -> \",accuracy_score(predictions_sgd, test_y)*100)\n",
    "    return sgd\n",
    "\n",
    "def Logistic_Regression_Classifier(train_x,train_y,test_x,test_y):\n",
    "    Logistic_Regression = LogisticRegression(n_jobs=1, C=1e5,max_iter=2000)\n",
    "    Logistic_Regression.fit(train_x,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_Logistic_Regression = Logistic_Regression.predict(test_x)\n",
    "    print(\"Logistic_Regression Accuracy Score -> \",accuracy_score(predictions_Logistic_Regression, test_y)*100)\n",
    "    return Logistic_Regression\n",
    "\n",
    "def MLP_Classifier(train_x,train_y,test_x,test_y):\n",
    "    mlp = MLPClassifier(solver='sgd', alpha=1e-5,\n",
    "                         hidden_layer_sizes=(5, 2), random_state=1,max_iter=20000)\n",
    "    \n",
    "    mlp.fit(train_x, train_y)\n",
    "    predictions_mlp = mlp.predict(test_x)\n",
    "    print(\"Multilayer Perceptron Accuracy Score -> \",accuracy_score(predictions_mlp, test_y)*100)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section I\n",
    "### Data Collection\n",
    "1. The required data here is extracted from data collected earlier \n",
    "2. The TOP, HOT and NEW posts are concatenated in frame \n",
    "3. Duplicates are removed \n",
    "4. Creating a text column which concatenates the title and body text \n",
    "5. Since the number of flare was large (40), all the flares with less than 3 posts are deleted here to improve model performance \n",
    "6. ALL the rows with NONE data are removed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text_flare_data = top_posts[['id','title','body','flare']]\n",
    "hot_text_flare_data = hot_posts[['id','title','body','flare']]\n",
    "new_text_flare_data = new_posts[['id','title','body','flare']]\n",
    "frames = [top_text_flare_data, hot_text_flare_data, new_text_flare_data]\n",
    "data = pd.concat(frames)\n",
    "\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.groupby('flare').filter(lambda x : len(x)>3)\n",
    "\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing, splitting into training and test data.<br>\n",
    "2. Encoding and vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = the entire text data , Y = entire target or flares\n",
    "X = data['text']\n",
    "Y = data['flare']\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y,test_size=0.3)\n",
    "\n",
    "Y,train_y,test_y, encoder = encode(Y,train_y,test_y)\n",
    "X,train_x,test_x, Tfidf_vect = tfidf(X,train_x,test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy of different ML algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = Naive_Bayes_Classifier(train_x, train_y, test_x, test_y)\n",
    "SVM = SVM_Classifier(train_x, train_y, test_x, test_y)\n",
    "Random_Forest = Random_Forest_Classifier(train_x, train_y, test_x, test_y)\n",
    "SGD =  SGD_Classifier(train_x, train_y, test_x, test_y)\n",
    "Logistic_Regression = Logistic_Regression_Classifier(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a confusion matrix to analyze the results from the best classifier yet (sgd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions_sgd = SGD.predict(test_x)\n",
    "cm = confusion_matrix(test_y, predictions_sgd)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II\n",
    "1. Since most of the cm is 0 or very less values here the dataset is reduced\n",
    "2. Now, only flares with greater than 100 posts are used.\n",
    "3. Data is already preprocessed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('flare').filter(lambda x : len(x)>100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data obtained above is divided into train,test and vectorized /encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = the entire text data , Y = entire target or flares\n",
    "X = data['text']\n",
    "Y = data['flare']\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y,test_size=0.3)\n",
    "Y,train_y,test_y, encoder = encode(Y,train_y,test_y)\n",
    "X,train_x,test_x, Tfidf_vect = tfidf(X,train_x,test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy is measured from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = Naive_Bayes_Classifier(train_x, train_y, test_x, test_y)\n",
    "SVM = SVM_Classifier(train_x, train_y, test_x, test_y)\n",
    "Random_Forest = Random_Forest_Classifier(train_x, train_y, test_x, test_y)\n",
    "SGD =  SGD_Classifier(train_x, train_y, test_x, test_y)\n",
    "Logistic_Regression = Logistic_Regression_Classifier(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3\n",
    "#### Data is oversampled\n",
    "( Data used is already encoded and vectorized here )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE('minority')\n",
    "X, Y = smote.fit_resample(X, Y)\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy is measured from the oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = Naive_Bayes_Classifier(train_x, train_y, test_x, test_y)\n",
    "SVM = SVM_Classifier(train_x, train_y, test_x, test_y)\n",
    "Random_Forest = Random_Forest_Classifier(train_x, train_y, test_x, test_y)\n",
    "SGD =  SGD_Classifier(train_x, train_y, test_x, test_y)\n",
    "Logistic_Regression = Logistic_Regression_Classifier(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions_sgd = SGD.predict(test_x)\n",
    "cm = confusion_matrix(test_y, predictions_sgd)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix indicates better values than previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4\n",
    "1. Using the Pushshift data, reddit r/india is scrapped for 2 years data.\n",
    "2. Here data is repeadtly scrapped in counts of 1000 posts from Jan 1 2018 to 10th April 2020.\n",
    "3. Calls to reddit API after every 1000 posts as that is the limit.\n",
    "4. This function takes a approx 10-20 minutes.\n",
    "5. Pandas dataframe is downloaded once the download finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "def getPushshiftData(after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "# This Function downloads data using pushshift API, from 'after' time to 'before' time, in 1000 posts iterations\n",
    "# Then after the data is collected it is made into a pandas dataframe which is downloaded so that the function is not used again \n",
    "# since this function takes a lot of time to execute\n",
    "def getData():\n",
    "    \n",
    "    subData = []\n",
    "    subCount = 0\n",
    "    #Subreddit to query\n",
    "    sub='india'\n",
    "    \n",
    "    #before and after dates\n",
    "    #unix time is provided\n",
    "    after = \"1514764800\"  #January 1st 2018\n",
    "    before = \"1586476800\" #10th April 2020\n",
    "\n",
    "    data = getPushshiftData(after, before, sub)\n",
    "\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            try:\n",
    "                flare = submission['link_flair_text']\n",
    "            except KeyError:\n",
    "                flare = None\n",
    "\n",
    "            if flare == None:\n",
    "                continue\n",
    "\n",
    "            sub_id = submission['id']\n",
    "            title = submission['title']\n",
    "            try:\n",
    "                selftext = submission['selftext']\n",
    "            except KeyError:\n",
    "                selftext = None\n",
    "\n",
    "            subData.append([sub_id,title,selftext,flare])    \n",
    "            subCount+=1\n",
    "        print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])),end = \" \")\n",
    "        #after every 1000 posts,after time is changed for API by using the last post extracted\n",
    "        after = data[-1]['created_utc']\n",
    "        data = getPushshiftData(after, before, sub)\n",
    "\n",
    "    original_expanded_data = pd.DataFrame(subData,columns=['id', 'title', 'body', 'flare'])\n",
    "    original_expanded_data.describe()\n",
    "    original_expanded_data.to_pickle(\"original_expanded_data.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the new expanded data flare count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data = pd.read_pickle(\"original_expanded_data.pkl\")\n",
    "expanded_data.drop_duplicates(keep='last',inplace=True)\n",
    "expanded_data['text'] = expanded_data['title'].str.cat(expanded_data['body'], sep =\" \")\n",
    "expanded_data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the flares which have 3000 posts +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data = expanded_data.groupby('flare').filter(lambda x : len(x)>3000)\n",
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "flare_class = expanded_data[\"flare\"].value_counts()\n",
    "flare_class.plot(kind= 'bar')\n",
    "plt.title('Count of different flares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data = expanded_data.mask(expanded_data.eq('None')).dropna()\n",
    "expanded_data['text'] = preprocessing(expanded_data['text'])\n",
    "expanded_data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding/vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = the entire text data , Y = entire target or flares\n",
    "X = expanded_data['text']\n",
    "Y = expanded_data['flare']\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y,test_size=0.3)\n",
    "X,train_x,test_x,Tfidf_vect = tfidf(X,train_x,test_x)\n",
    "Y,train_y,test_y,encoder = encode(Y,train_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling the Data since data is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "print('Original dataset shape %s' % Counter(Y))\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X, Y = rus.fit_resample(X, Y)\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y,test_size=0.3)\n",
    "\n",
    "print(Counter(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Accuracy from this data\n",
    "SVM and logistic regression are commented out since they take a lot of time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive = Naive_Bayes_Classifier(train_x, train_y, test_x, test_y)\n",
    "Random_Forest = Random_Forest_Classifier(train_x, train_y, test_x, test_y)\n",
    "SGD =  SGD_Classifier(train_x, train_y, test_x, test_y)\n",
    "SVM = SVM_Classifier(train_x, train_y, test_x, test_y)\n",
    "Logistic_Regression = Logistic_Regression_Classifier(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions_SVM = SVM.predict(test_x)\n",
    "cm = confusion_matrix(test_y, predictions_SVM)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below code will download the model, tfidf vectorizer and encoder to be directly used in Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Using the trained Naive model\n",
    "model = SVM\n",
    "pickle.dump(model, open(\"model.pkl\",\"wb\"))\n",
    "\n",
    "# Taking the TfIDF and encoder\n",
    "pickle.dump(Tfidf_vect, open(\"tfidf.pickle\", \"wb\"))\n",
    "np.save('classes.npy', encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is used to test the endpoint by sending request from test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with open('test.txt', 'rb') as f:\n",
    "    r = requests.post('https://reddit-flare-detector.herokuapp.com/automated_testing', files={'test.txt': f})\n",
    "    a = r.json()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on Manual Examples\n",
    "below example list is created on which the models will predict the flare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = np.array([\"Stuck in quarantine. Playing games all day\",\"This. virus is killing me\",\"Government of India needs to take action \",\"All the political ministers need to work for india\"])\n",
    "Example = pd.Series(example_list)\n",
    "Example = preprocessing(Example)\n",
    "Example_Tfidf = Tfidf_vect.transform(Example)\n",
    "model = SVM\n",
    "predict_example = model.predict(Example_Tfidf)\n",
    "print(encoder.inverse_transform(predict_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
