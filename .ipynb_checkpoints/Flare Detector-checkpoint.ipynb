{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer    \n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "set(stopwords.words('english'))\n",
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading lib\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Id to scrape\n",
    "my_client_id = \"tlaYd7tsDOqvlQ\"\n",
    "my_client_secret = \"xdRqwLkA07r8ScyJZTMsYUndrSA\"\n",
    "my_user_agent = \"scrapping r/india\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to print full panda data frame to analyze data\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_colwidth', 950)\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API to access reddit data\n",
    "reddit = praw.Reddit(client_id=my_client_id, client_secret=my_client_secret, user_agent=my_user_agent)\n",
    "india_subreddit = reddit.subreddit('India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "<ul>\n",
    "    <li>Getting TOP, HOT and NEW posts of reddit using the praw API </li>\n",
    "    <li>Many JSON tags are retrived which may or may not be used in future </li>\n",
    "    <li>https://github.com/reddit-archive/reddit/wiki/JSON used to study the tags </li>\n",
    "    <li>This is done to get all the types of posts and not be trained on only one type. Also increases training data</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = []\n",
    "for post in india_subreddit.top(limit=1000):\n",
    "    top_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "\n",
    "hot_posts = []\n",
    "for post in india_subreddit.hot(limit=1000):\n",
    "    hot_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "\n",
    "\n",
    "new_posts = []\n",
    "for post in india_subreddit.new(limit=1000):\n",
    "    new_posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Converting the data obtained into data frames for analyzing and combining the three types of data with duplicates removed</li>\n",
    "    <li>Adding a text column that is title + body</li>\n",
    "    <li>Removing NONE instances</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = pd.DataFrame(top_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "hot_posts = pd.DataFrame(hot_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "new_posts = pd.DataFrame(new_posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','flare'])\n",
    "\n",
    "frames = [top_posts, hot_posts, new_posts]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we analyze the division of count of posts in respective flares\n",
    "This will tell how many and which flares are prominent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "flare_class = data[\"flare\"].value_counts()\n",
    "flare_class.plot(kind= 'bar')\n",
    "plt.title('Count of different flares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the word cloud of the four main Flares - Coronavirus, non-political, politics, askindia\n",
    "This is done to visualize which words are more used wrt flares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "Coronavirus = data[data[\"flare\"] == \"Coronavirus\"]\n",
    "Non_Political = data[data[\"flare\"] == \"Non-Political\"]\n",
    "Politics = data[data[\"flare\"] == \"Politics\"]\n",
    "AskIndia = data[data[\"flare\"] == \"AskIndia\"]\n",
    "\n",
    "Coronavirus_words = ''\n",
    "Non_Political_words = ''\n",
    "Politics_words = ''\n",
    "AskIndia_words = ''\n",
    "\n",
    "#Extracting words wrt flares\n",
    "\n",
    "for t in Coronavirus.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Coronavirus_words = Coronavirus_words + w + ' '\n",
    "        \n",
    "for t in Non_Political.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Non_Political_words = Non_Political_words + w + ' '\n",
    "        \n",
    "for t in Politics.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        Politics_words = Politics_words + w + ' '\n",
    "        \n",
    "for t in AskIndia.text:\n",
    "    text = t.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    for w in tokens:\n",
    "        AskIndia_words = AskIndia_words + w + ' '        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coronavirus Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Coronavirus_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-politcal Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Non_Political_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Politics words Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(Politics_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask India words, Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=600, height=400).generate(AskIndia_words)\n",
    "#Insincere Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of length of posts and number of words used in different flare posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coronavirus[\"post_length\"] = Coronavirus.text.apply(lambda x: len(x))\n",
    "Non_Political[\"post_length\"] = Non_Political.text.apply(lambda x: len(x))\n",
    "\n",
    "Coronavirus['number_words'] = Coronavirus.text.apply(lambda x: len(x.split()))\n",
    "Non_Political['number_words'] = Non_Political.text.apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Mean Length of Coronavirus Posts \" + str(Coronavirus[\"post_length\"].mean()))\n",
    "print(\"Mean Length of Non Political Posts \"  + str(Non_Political[\"post_length\"].mean()))\n",
    "print(\"Mean word usage of Coronavirus Posts \"  + str(Coronavirus['number_words'].mean()))\n",
    "print(\"Mean word usage of Non Political Posts \"  + str(Non_Political['number_words'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Flare Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function to build confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "<ul>\n",
    "    <li> The required data here is extracted from data collected earlier </li>\n",
    "    <li> The TOP, HOT and NEW posts are concatenated in frame </li>\n",
    "    <li> duplicates are removed </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text_flare_data = top_posts[['id','title','body','flare']]\n",
    "hot_text_flare_data = hot_posts[['id','title','body','flare']]\n",
    "new_text_flare_data = new_posts[['id','title','body','flare']]\n",
    "\n",
    "frames = [top_text_flare_data, hot_text_flare_data, new_text_flare_data]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I\n",
    "Here around 15 flares are considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the number of flare was large (40), all the flares with less than 3 posts are deleted here.\n",
    "<ul>\n",
    "<li>This is done since having large number of flares reduced the accuracy if test dataset on splitting only had the lower ones.</li>\n",
    "<li>Also model is able to predict only the most prominent ones due to less data. </li>\n",
    "<li>Creating a text column which concatenates the title and body text </li>\n",
    "<li> ALL the rows with NONE data are removed </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('flare').filter(lambda x : len(x)>3)\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "<ul>\n",
    "    <li>Creating regex emoji pattern to remove emojis</li>\n",
    "    <li> All strings are lowered </li>\n",
    "    <li> Punctuation, whitespaces,links are removed</li>\n",
    "    <li> Stop words are removed and words are stemmed</li>\n",
    "    <li> Finally it contains list of strings where each word is stemmed and cleaned</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['text'] = (data['text'].str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in data['text']:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "data['text'] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['text'], data['flare'],test_size=0.3)\n",
    "\n",
    "# X = the entire text data , Y = entire target or flares\n",
    "X = data['text']\n",
    "Y = data['flare']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# label encode the target variable \n",
    "Y = encoder.fit_transform(Y)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the words to vectors using Tfidf to be used for ML algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
    "X = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy of different ML algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_naive = accuracy_score(predictions_NB, test_y)*100\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_SVM = accuracy_score(predictions_SVM, test_y)*100\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "random_forest_classfier = ensemble.RandomForestClassifier()\n",
    "random_forest_classfier.fit(Train_X_Tfidf,train_y)\n",
    "predictions_RF = random_forest_classfier.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_rfc = accuracy_score(predictions_RF, test_y)*100\n",
    "print(\"RF Accuracy Score -> \",accuracy_score(predictions_RF, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGDClassifier \n",
    "linear classifier optimized by the SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "sgd.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_sgd = sgd.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_sgd = accuracy_score(predictions_sgd, test_y)*100\n",
    "print(\"SGD Accuracy Score -> \",accuracy_score(predictions_sgd, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logistic_Regression = LogisticRegression(n_jobs=1, C=1e5,max_iter=2000)\n",
    "Logistic_Regression.fit(Train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_Logistic_Regression = Logistic_Regression.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "accuracy_lr = accuracy_score(predictions_Logistic_Regression, test_y)*100\n",
    "print(\"Logistic_Regression Accuracy Score -> \",accuracy_score(predictions_Logistic_Regression, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = [accuracy_naive,accuracy_SVM,accuracy_rfc,accuracy_sgd,accuracy_lr]\n",
    "model_names = [\"Naive\",\"SVM\",\"random_forest_classfier\",\"sgd\",\"Logistic_Regression\"]\n",
    "for i in range(0,len(models)):\n",
    "    print(\"Accuracy by \" + model_names[i] + \" = \" + str(model_prediction[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "model = ensemble.RandomForestClassifier()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on Manual Examples\n",
    "below example list is created on which the models will predict the flare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = np.array([\"Stuck in quarantine. Playing games all day\",\"This. virus is killing me\",\"Government of India needs to take action \",\"All the political ministers need to work for india\"])\n",
    "Example = pd.Series(example_list)\n",
    "Example = (Example.str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in Example:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "Example = res\n",
    "Example_Tfidf = Tfidf_vect.transform(Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Naive,SVM,random_forest_classfier,sgd,Logistic_Regression]\n",
    "for i in range(0,len(models)):\n",
    "    model = models[i]\n",
    "    predict_example = model.predict(Example_Tfidf)\n",
    "    print(\"Predictions by \" + model_names[i])\n",
    "    print(encoder.inverse_transform(predict_example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a confusion matrix to analyze the results from the best classifier yet (sgd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_y, predictions_sgd)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II\n",
    "Since most of the cm is 0 or very less values here the dataset is reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to get accuracy from all the ML models which were defined as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(Train_X_Tfidf,train_y,Test_X_Tfidf,test_y):\n",
    "    from sklearn import naive_bayes\n",
    "    # fit the training dataset on the NB classifier\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_naive = accuracy_score(predictions_NB, test_y)*100\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y)*100)\n",
    "\n",
    "    from sklearn import svm\n",
    "    # fit the training dataset on the classifier\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_SVM = accuracy_score(predictions_SVM, test_y)*100\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)\n",
    "\n",
    "    from sklearn import ensemble\n",
    "    random_forest_classfier = ensemble.RandomForestClassifier()\n",
    "    random_forest_classfier.fit(Train_X_Tfidf,train_y)\n",
    "    predictions_RF = random_forest_classfier.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_rfc = accuracy_score(predictions_RF, test_y)*100\n",
    "    print(\"RF Accuracy Score -> \",accuracy_score(predictions_RF, test_y)*100)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    Logistic_Regression = LogisticRegression(n_jobs=1, C=1e5,max_iter=2000)\n",
    "    Logistic_Regression.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_Logistic_Regression = Logistic_Regression.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_lr = accuracy_score(predictions_Logistic_Regression, test_y)*100\n",
    "    print(\"Logistic_Regression Accuracy Score -> \",accuracy_score(predictions_Logistic_Regression, test_y)*100)\n",
    "\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    sgd.fit(Train_X_Tfidf,train_y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_sgd = sgd.predict(Test_X_Tfidf)\n",
    "    # Use accuracy_score function to get the accuracy\n",
    "    accuracy_sgd = accuracy_score(predictions_sgd, test_y)*100\n",
    "    print(\"SGD Accuracy Score -> \",accuracy_score(predictions_sgd, test_y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is made again by using the scraped data, but here only flares with greater than 100 posts are used\n",
    "<ul>\n",
    "    <li>Data is cleaned and prepared</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text_flare_data = top_posts[['id','title','body','flare']]\n",
    "hot_text_flare_data = hot_posts[['id','title','body','flare']]\n",
    "new_text_flare_data = new_posts[['id','title','body','flare']]\n",
    "\n",
    "frames = [top_text_flare_data, hot_text_flare_data, new_text_flare_data]\n",
    "data = pd.concat(frames)\n",
    "data.drop_duplicates(keep='last',inplace=True)\n",
    "data = data.groupby('flare').filter(lambda x : len(x)>100)\n",
    "\n",
    "data['text'] = data['title'].str.cat(data['body'], sep =\" \")\n",
    "data = data.mask(data.eq('None')).dropna()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data['text'] = (data['text'].str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in data['text']:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "data['text'] = res\n",
    "data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data obtained above is divided into train,test and vectorized /encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['text'], data['flare'],test_size=0.3)\n",
    "\n",
    "# X = the entire text data , Y = entire target or flares\n",
    "X = data['text']\n",
    "Y = data['flare']\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# label encode the target variable \n",
    "Y = encoder.fit_transform(Y)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n",
    "Tfidf_vect = TfidfVectorizer(max_features=15000)\n",
    "Tfidf_vect.fit(data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
    "X = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy is measured from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(Train_X_Tfidf,train_y,Test_X_Tfidf,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE('minority')\n",
    "X_oversampled, Y_oversampled = smote.fit_resample(X, Y)\n",
    "Train_X_Tfidf_oversampled, Test_X_Tfidf_oversampled, train_y_oversampled, test_y_oversampled = model_selection.train_test_split(X_oversampled, Y_oversampled,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy is measured from the oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(Train_X_Tfidf_oversampled,train_y_oversampled,Test_X_Tfidf_oversampled,test_y_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "model.fit(Train_X_Tfidf_oversampled,train_y_oversampled)\n",
    "predictions_sgd = model.predict(Test_X_Tfidf_oversampled)\n",
    "cm = confusion_matrix(test_y_oversampled, predictions_sgd)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=[],normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix indicates better values than previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III\n",
    "Increasing the amount of data by a large count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the Pushshift data, reddit r/india is scrapped for 2 years data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the helper function for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "def getPushshiftData(after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "   # print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here data is repeadtly scrapped in counts of 1000 posts from Jan 1 2018 to 10th April 2020.\n",
    "### Calls to reddit API after every 1000 posts as that is the limit\n",
    "#### This function takes a approx 10-20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-03 00:42:23\n",
      "2018-01-04 17:22:35\n",
      "2018-01-06 12:20:39\n",
      "2018-01-08 12:57:11\n",
      "2018-01-10 00:09:16\n",
      "2018-01-11 16:03:05\n",
      "2018-01-13 11:40:02\n",
      "2018-01-15 14:46:07\n",
      "2018-01-17 09:46:49\n",
      "2018-01-18 20:04:25\n",
      "2018-01-20 13:57:37\n",
      "2018-01-22 12:58:49\n",
      "2018-01-24 02:15:41\n",
      "2018-01-25 19:07:34\n",
      "2018-01-27 20:21:03\n",
      "2018-01-30 06:56:17\n",
      "2018-01-31 21:32:31\n",
      "2018-02-02 19:22:28\n",
      "2018-02-05 09:45:09\n",
      "2018-02-07 07:51:30\n",
      "2018-02-08 22:32:00\n",
      "2018-02-10 22:27:33\n",
      "2018-02-12 22:40:08\n",
      "2018-02-14 17:32:23\n",
      "2018-02-16 11:44:15\n",
      "2018-02-18 11:39:58\n",
      "2018-02-20 11:36:05\n",
      "2018-02-22 07:01:22\n",
      "2018-02-23 23:08:25\n",
      "2018-02-26 00:57:38\n",
      "2018-02-27 19:37:45\n",
      "2018-03-01 14:03:23\n",
      "2018-03-03 14:34:39\n",
      "2018-03-05 19:00:23\n",
      "2018-03-07 15:09:59\n",
      "2018-03-09 10:36:12\n",
      "2018-03-11 11:17:14\n",
      "2018-03-13 08:45:09\n",
      "2018-03-14 20:55:15\n",
      "2018-03-16 17:00:37\n",
      "2018-03-18 16:57:12\n",
      "2018-03-20 14:20:18\n",
      "2018-03-22 09:24:33\n",
      "2018-03-23 20:05:57\n",
      "2018-03-25 20:37:12\n",
      "2018-03-27 15:31:39\n",
      "2018-03-29 07:00:41\n",
      "2018-03-30 23:49:03\n",
      "2018-04-02 08:21:20\n",
      "2018-04-03 20:56:51\n",
      "2018-04-05 14:19:05\n",
      "2018-04-07 07:52:27\n",
      "2018-04-09 10:02:16\n",
      "2018-04-10 23:08:10\n",
      "2018-04-12 16:40:08\n",
      "2018-04-14 09:54:14\n",
      "2018-04-16 05:09:31\n",
      "2018-04-17 18:17:48\n",
      "2018-04-19 12:59:33\n",
      "2018-04-21 00:38:07\n",
      "2018-04-23 08:12:47\n",
      "2018-04-24 21:44:59\n",
      "2018-04-26 21:20:17\n",
      "2018-04-29 12:02:35\n",
      "2018-05-01 13:25:11\n",
      "2018-05-03 15:46:15\n",
      "2018-05-05 22:12:19\n",
      "2018-05-08 10:26:24\n",
      "2018-05-10 10:39:50\n",
      "2018-05-12 14:18:16\n",
      "2018-05-14 19:37:06\n",
      "2018-05-17 09:42:46\n",
      "2018-05-19 14:34:06\n",
      "2018-05-22 08:13:42\n",
      "2018-05-24 09:24:32\n",
      "2018-05-26 12:46:44\n",
      "2018-05-28 19:59:25\n",
      "2018-05-30 20:29:06\n",
      "2018-06-01 20:02:39\n",
      "2018-06-04 09:31:22\n",
      "2018-06-06 08:31:14\n",
      "2018-06-08 09:16:03\n",
      "2018-06-10 13:04:38\n",
      "2018-06-12 16:01:24\n",
      "2018-06-14 17:44:40\n",
      "2018-06-17 01:57:54\n",
      "2018-06-19 14:56:26\n",
      "2018-06-21 18:54:08\n",
      "2018-06-24 00:37:38\n",
      "2018-06-26 12:38:13\n",
      "2018-06-28 12:20:30\n",
      "2018-06-30 12:59:09\n",
      "2018-07-02 22:56:18\n",
      "2018-07-05 00:37:31\n",
      "2018-07-06 22:33:57\n",
      "2018-07-09 12:44:47\n",
      "2018-07-11 10:26:11\n",
      "2018-07-13 06:30:04\n",
      "2018-07-15 11:56:56\n",
      "2018-07-17 14:25:22\n",
      "2018-07-19 14:02:42\n",
      "2018-07-21 08:40:47\n",
      "2018-07-23 10:46:10\n",
      "2018-07-25 10:24:21\n",
      "2018-07-27 10:05:28\n",
      "2018-07-29 15:42:38\n",
      "2018-07-31 14:59:41\n",
      "2018-08-02 18:33:57\n",
      "2018-08-05 07:32:23\n",
      "2018-08-07 10:54:59\n",
      "2018-08-09 10:51:38\n",
      "2018-08-11 10:36:02\n",
      "2018-08-13 17:27:52\n",
      "2018-08-15 14:11:48\n",
      "2018-08-17 11:38:28\n",
      "2018-08-19 15:23:57\n",
      "2018-08-21 13:14:33\n",
      "2018-08-23 03:25:31\n",
      "2018-08-24 20:35:40\n",
      "2018-08-27 04:25:00\n",
      "2018-08-28 18:49:26\n",
      "2018-08-30 13:14:02\n",
      "2018-09-01 08:11:06\n",
      "2018-09-03 10:08:30\n",
      "2018-09-04 20:32:10\n",
      "2018-09-06 17:51:52\n",
      "2018-09-09 12:09:35\n",
      "2018-09-11 21:08:30\n",
      "2018-09-13 20:30:32\n",
      "2018-09-15 20:08:38\n",
      "2018-09-17 21:11:17\n",
      "2018-09-19 18:19:38\n",
      "2018-09-21 14:52:53\n",
      "2018-09-23 15:43:08\n",
      "2018-09-25 13:00:31\n",
      "2018-09-26 22:52:44\n",
      "2018-09-28 16:31:46\n",
      "2018-09-30 16:32:29\n",
      "2018-10-02 13:36:21\n",
      "2018-10-04 10:58:05\n",
      "2018-10-06 01:07:15\n",
      "2018-10-08 01:19:33\n",
      "2018-10-09 15:19:57\n",
      "2018-10-11 08:36:15\n",
      "2018-10-12 18:27:36\n",
      "2018-10-14 17:44:05\n",
      "2018-10-16 15:25:11\n",
      "2018-10-18 11:23:47\n",
      "2018-10-20 08:17:51\n",
      "2018-10-22 11:25:50\n",
      "2018-10-23 22:29:31\n",
      "2018-10-25 15:54:25\n",
      "2018-10-27 11:10:08\n",
      "2018-10-29 11:27:34\n",
      "2018-10-30 23:43:17\n",
      "2018-11-01 15:33:05\n",
      "2018-11-03 11:35:20\n",
      "2018-11-05 10:15:07\n",
      "2018-11-06 21:52:16\n",
      "2018-11-08 19:33:16\n",
      "2018-11-10 18:24:27\n",
      "2018-11-12 17:11:39\n",
      "2018-11-14 12:25:22\n",
      "2018-11-16 07:46:34\n",
      "2018-11-18 08:49:07\n",
      "2018-11-20 09:38:39\n",
      "2018-11-21 20:32:42\n",
      "2018-11-23 17:02:32\n",
      "2018-11-25 18:01:00\n",
      "2018-11-27 15:31:28\n",
      "2018-11-29 11:30:48\n",
      "2018-12-01 06:37:24\n",
      "2018-12-03 11:08:36\n",
      "2018-12-05 01:42:24\n",
      "2018-12-06 21:48:52\n",
      "2018-12-08 22:51:43\n",
      "2018-12-11 06:35:47\n",
      "2018-12-12 19:49:43\n",
      "2018-12-14 17:02:07\n",
      "2018-12-16 19:28:03\n",
      "2018-12-18 18:42:11\n",
      "2018-12-20 19:33:08\n",
      "2018-12-22 19:22:08\n",
      "2018-12-25 09:24:28\n",
      "2018-12-27 10:32:37\n",
      "2018-12-29 09:28:02\n",
      "2018-12-31 16:26:54\n",
      "2019-01-02 18:01:16\n",
      "2019-01-04 15:08:30\n",
      "2019-01-06 19:11:20\n",
      "2019-01-08 17:46:39\n",
      "2019-01-10 15:20:42\n",
      "2019-01-12 14:21:08\n",
      "2019-01-14 18:01:50\n",
      "2019-01-16 21:03:23\n",
      "2019-01-18 21:10:51\n",
      "2019-01-21 12:37:46\n",
      "2019-01-23 12:32:39\n",
      "2019-01-25 13:36:17\n",
      "2019-01-27 19:14:32\n",
      "2019-01-29 21:29:17\n",
      "2019-01-31 18:42:16\n",
      "2019-02-02 16:03:16\n",
      "2019-02-04 19:33:47\n",
      "2019-02-06 19:13:38\n",
      "2019-02-08 16:42:28\n",
      "2019-02-10 20:52:36\n",
      "2019-02-12 20:37:46\n",
      "2019-02-14 17:34:28\n",
      "2019-02-16 12:23:56\n",
      "2019-02-18 11:47:14\n",
      "2019-02-20 01:01:24\n",
      "2019-02-21 19:43:48\n",
      "2019-02-23 16:34:30\n",
      "2019-02-25 17:20:16\n",
      "2019-02-27 05:38:34\n",
      "2019-02-28 10:40:17\n",
      "2019-03-01 14:37:39\n",
      "2019-03-03 02:17:27\n",
      "2019-03-04 21:32:18\n",
      "2019-03-06 13:30:50\n",
      "2019-03-08 01:36:13\n",
      "2019-03-09 22:20:22\n",
      "2019-03-11 21:01:43\n",
      "2019-03-13 09:47:16\n",
      "2019-03-14 15:58:00\n",
      "2019-03-16 10:38:15\n",
      "2019-03-18 08:28:10\n",
      "2019-03-19 20:15:19\n",
      "2019-03-21 14:26:56\n",
      "2019-03-23 10:50:08\n",
      "2019-03-25 13:13:45\n",
      "2019-03-27 10:21:24\n",
      "2019-03-28 22:20:38\n",
      "2019-03-30 20:43:38\n",
      "2019-04-01 17:09:35\n",
      "2019-04-03 11:15:58\n",
      "2019-04-05 01:22:14\n",
      "2019-04-06 23:21:35\n",
      "2019-04-08 21:56:00\n",
      "2019-04-10 15:11:30\n",
      "2019-04-12 07:27:09\n",
      "2019-04-13 21:43:07\n",
      "2019-04-15 21:45:12\n",
      "2019-04-17 16:36:31\n",
      "2019-04-19 11:38:59\n",
      "2019-04-21 10:06:30\n",
      "2019-04-23 07:57:14\n",
      "2019-04-24 22:42:51\n",
      "2019-04-26 19:54:09\n",
      "2019-04-28 20:32:22\n",
      "2019-04-30 17:07:55\n",
      "2019-05-02 13:47:06\n",
      "2019-05-04 11:11:16\n",
      "2019-05-06 12:16:24\n",
      "2019-05-08 10:01:08\n",
      "2019-05-10 00:47:01\n",
      "2019-05-11 20:32:10\n",
      "2019-05-13 20:00:28\n",
      "2019-05-15 16:44:48\n",
      "2019-05-17 11:59:26\n",
      "2019-05-19 12:05:39\n",
      "2019-05-21 11:42:08\n",
      "2019-05-23 01:36:04\n",
      "2019-05-24 12:23:44\n",
      "2019-05-26 11:00:57\n",
      "2019-05-28 08:50:57\n",
      "2019-05-29 19:48:06\n",
      "2019-05-31 15:19:29\n",
      "2019-06-02 17:17:52\n",
      "2019-06-04 16:24:49\n",
      "2019-06-06 13:33:56\n",
      "2019-06-08 10:55:56\n",
      "2019-06-10 13:25:19\n",
      "2019-06-12 05:45:49\n",
      "2019-06-13 18:28:14\n",
      "2019-06-15 15:50:30\n",
      "2019-06-17 17:21:30\n",
      "2019-06-19 14:20:39\n",
      "2019-06-21 10:34:39\n",
      "2019-06-23 09:43:36\n",
      "2019-06-25 02:33:38\n",
      "2019-06-26 20:50:53\n",
      "2019-06-28 16:12:36\n",
      "2019-06-30 17:20:06\n",
      "2019-07-02 17:06:44\n",
      "2019-07-04 13:49:46\n",
      "2019-07-06 13:33:51\n",
      "2019-07-08 18:20:04\n",
      "2019-07-10 16:10:02\n",
      "2019-07-12 15:05:36\n",
      "2019-07-14 20:24:54\n",
      "2019-07-16 19:36:15\n",
      "2019-07-19 08:49:34\n",
      "2019-07-21 15:17:29\n",
      "2019-07-23 18:29:37\n",
      "2019-07-25 16:23:50\n",
      "2019-07-27 17:36:35\n",
      "2019-07-29 18:25:40\n",
      "2019-07-31 15:27:20\n",
      "2019-08-02 15:12:24\n",
      "2019-08-04 20:27:42\n",
      "2019-08-06 15:39:47\n",
      "2019-08-08 14:11:26\n",
      "2019-08-10 13:04:26\n",
      "2019-08-12 13:39:31\n",
      "2019-08-14 13:48:10\n",
      "2019-08-16 10:52:38\n",
      "2019-08-18 12:34:38\n",
      "2019-08-20 14:36:23\n",
      "2019-08-22 14:16:39\n",
      "2019-08-24 13:00:50\n",
      "2019-08-26 20:03:08\n",
      "2019-08-28 19:08:58\n",
      "2019-08-30 23:39:59\n",
      "2019-09-02 10:24:00\n",
      "2019-09-04 10:07:28\n",
      "2019-09-06 13:01:48\n",
      "2019-09-08 15:12:32\n",
      "2019-09-10 16:39:48\n",
      "2019-09-12 16:37:06\n",
      "2019-09-14 21:27:16\n",
      "2019-09-17 11:39:05\n",
      "2019-09-19 00:49:20\n",
      "2019-09-20 21:10:42\n",
      "2019-09-23 02:18:58\n",
      "2019-09-25 02:41:39\n",
      "2019-09-26 20:44:36\n",
      "2019-09-29 00:18:14\n",
      "2019-10-01 02:43:52\n",
      "2019-10-03 02:53:57\n",
      "2019-10-05 09:08:23\n",
      "2019-10-07 15:46:57\n",
      "2019-10-09 17:33:09\n",
      "2019-10-11 15:37:09\n",
      "2019-10-13 21:06:10\n",
      "2019-10-15 19:34:26\n",
      "2019-10-17 20:33:56\n",
      "2019-10-19 22:12:29\n",
      "2019-10-22 09:11:38\n",
      "2019-10-24 10:42:26\n",
      "2019-10-26 16:22:39\n",
      "2019-10-28 22:02:29\n",
      "2019-10-30 19:57:09\n",
      "2019-11-02 12:16:17\n",
      "2019-11-04 23:52:37\n",
      "2019-11-07 05:44:14\n",
      "2019-11-09 11:43:11\n",
      "2019-11-12 00:59:12\n",
      "2019-11-14 21:02:24\n",
      "2019-11-17 19:19:34\n",
      "2019-11-20 14:46:43\n",
      "2019-11-23 09:42:26\n",
      "2019-11-26 09:26:48\n",
      "2019-11-28 17:12:24\n",
      "2019-12-01 09:42:07\n",
      "2019-12-03 19:57:30\n",
      "2019-12-06 10:10:46\n",
      "2019-12-08 20:42:58\n",
      "2019-12-11 10:49:39\n",
      "2019-12-13 11:08:16\n",
      "2019-12-15 17:33:17\n",
      "2019-12-16 21:02:55\n",
      "2019-12-18 00:14:29\n",
      "2019-12-19 10:59:30\n",
      "2019-12-20 01:52:21\n",
      "2019-12-21 06:40:23\n",
      "2019-12-22 14:43:20\n",
      "2019-12-23 21:56:26\n",
      "2019-12-25 21:30:01\n",
      "2019-12-27 17:20:03\n",
      "2019-12-29 20:57:21\n",
      "2019-12-31 18:27:44\n",
      "2020-01-02 20:41:02\n",
      "2020-01-05 01:08:45\n",
      "2020-01-06 21:24:54\n",
      "2020-01-08 16:34:21\n",
      "2020-01-10 12:49:45\n",
      "2020-01-12 12:58:11\n",
      "2020-01-14 14:56:49\n",
      "2020-01-16 16:49:18\n",
      "2020-01-19 08:14:25\n",
      "2020-01-21 16:30:15\n",
      "2020-01-24 04:00:30\n",
      "2020-01-26 16:33:24\n",
      "2020-01-29 02:41:37\n",
      "2020-01-30 23:20:59\n",
      "2020-02-02 01:39:32\n",
      "2020-02-04 14:33:29\n",
      "2020-02-06 19:31:01\n",
      "2020-02-09 11:12:52\n",
      "2020-02-11 23:27:24\n",
      "2020-02-14 13:05:45\n",
      "2020-02-17 11:00:05\n",
      "2020-02-19 20:50:52\n",
      "2020-02-22 16:30:33\n",
      "2020-02-25 09:58:21\n",
      "2020-02-27 01:32:37\n",
      "2020-02-28 21:06:05\n",
      "2020-03-02 07:53:11\n",
      "2020-03-04 12:40:00\n",
      "2020-03-06 17:39:35\n",
      "2020-03-09 07:47:18\n",
      "2020-03-11 22:14:57\n",
      "2020-03-14 08:09:10\n",
      "2020-03-16 14:35:37\n",
      "2020-03-18 16:24:11\n",
      "2020-03-20 17:48:06\n",
      "2020-03-22 09:17:16\n",
      "2020-03-23 01:54:37\n",
      "2020-03-24 14:24:51\n",
      "2020-03-25 18:33:05\n",
      "2020-03-27 12:02:54\n",
      "2020-03-28 21:25:24\n",
      "2020-03-30 13:10:32\n",
      "2020-03-31 23:00:29\n",
      "2020-04-02 15:35:07\n",
      "2020-04-03 22:31:42\n",
      "2020-04-05 16:35:54\n",
      "2020-04-06 19:33:06\n",
      "2020-04-08 15:45:49\n",
      "2020-04-10 05:29:51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>421738</td>\n",
       "      <td>421738</td>\n",
       "      <td>420839</td>\n",
       "      <td>224493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>421738</td>\n",
       "      <td>401141</td>\n",
       "      <td>54954</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>8w0603</td>\n",
       "      <td>Late Night Random Discussion Thread !</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>328916</td>\n",
       "      <td>60969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                  title    body     flare\n",
       "count   421738                                 421738  420839    224493\n",
       "unique  421738                                 401141   54954       222\n",
       "top     8w0603  Late Night Random Discussion Thread !          Politics\n",
       "freq         1                                    249  328916     60969"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subData = []\n",
    "subCount = 0\n",
    "#Subreddit to query\n",
    "sub='india'\n",
    "#before and after dates\n",
    "\n",
    "#unix time is provided\n",
    "after = \"1514764800\"  #January 1st 2018\n",
    "before = \"1586476800\" #10th April 2020\n",
    "\n",
    "data = getPushshiftData(after, before, sub)\n",
    "\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        try:\n",
    "            flare = submission['link_flair_text']\n",
    "        except KeyError:\n",
    "            flare = None\n",
    "         \n",
    "        if flare == None:\n",
    "            continue\n",
    "            \n",
    "        sub_id = submission['id']\n",
    "        title = submission['title']\n",
    "        try:\n",
    "            selftext = submission['selftext']\n",
    "        except KeyError:\n",
    "            selftext = None\n",
    "        \n",
    "        subData.append([sub_id,title,selftext,flare])    \n",
    "        subCount+=1\n",
    "   # print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    #after every 1000 posts,after time is changed for API by using the last post extracted\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftData(after, before, sub)\n",
    "              \n",
    "original_expanded_data = pd.DataFrame(subData,columns=['id', 'title', 'body', 'flare'])\n",
    "original_expanded_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                                             60904\n",
       "Non-Political                                        59430\n",
       "AskIndia                                             32861\n",
       "Business/Finance                                     13169\n",
       "Science/Technology                                   10245\n",
       "                                                     ...  \n",
       "| Repost | Post link Directly                            1\n",
       "Self-promotion. Not Original Title.                      1\n",
       "Personal Info. Custom; Informed OP.                      1\n",
       "| Not specific to India | Low Quality/Non OC Meme        1\n",
       "Megathread                                               1\n",
       "Name: flare, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_data = original_expanded_data\n",
    "expanded_data.drop_duplicates(keep='last',inplace=True)\n",
    "expanded_data = expanded_data[pd.notnull(expanded_data['flare'])]\n",
    "expanded_data['text'] = expanded_data['title'].str.cat(expanded_data['body'], sep =\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                                             60904\n",
       "Non-Political                                        59430\n",
       "AskIndia                                             32861\n",
       "Business/Finance                                     13169\n",
       "Science/Technology                                   10245\n",
       "                                                     ...  \n",
       "| Repost | Post link Directly                            1\n",
       "Self-promotion. Not Original Title.                      1\n",
       "Personal Info. Custom; Informed OP.                      1\n",
       "| Not specific to India | Low Quality/Non OC Meme        1\n",
       "Megathread                                               1\n",
       "Name: flare, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_data['flare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_data = expanded_data.groupby('flare').filter(lambda x : len(x)>3000)\n",
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "flare_class = expanded_data[\"flare\"].value_counts()\n",
    "flare_class.plot(kind= 'bar')\n",
    "plt.title('Count of different flares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>flare</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "      <td>210245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>210245</td>\n",
       "      <td>200593</td>\n",
       "      <td>34841</td>\n",
       "      <td>11</td>\n",
       "      <td>199121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>b7auxa</td>\n",
       "      <td>Slag Crusher Plant Manufacturer Exporters in I...</td>\n",
       "      <td></td>\n",
       "      <td>Politics</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>153490</td>\n",
       "      <td>60904</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title    body  \\\n",
       "count   210245                                             210245  210245   \n",
       "unique  210245                                             200593   34841   \n",
       "top     b7auxa  Slag Crusher Plant Manufacturer Exporters in I...           \n",
       "freq         1                                                131  153490   \n",
       "\n",
       "           flare    text  \n",
       "count     210245  210245  \n",
       "unique        11  199121  \n",
       "top     Politics          \n",
       "freq       60904     242  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "\n",
    "expanded_data['text'] = (expanded_data['text'].str.lower() #lowercase\n",
    "                           .str.replace(r'[^\\w\\s]+', '') #rem punctuation \n",
    "                           .str.replace(emoji_pattern, '') #rem emoji\n",
    "                           .str.replace(r'http\\S+','') #rem links\n",
    "                           .str.strip() #rem trailing whitespaces\n",
    "                           .str.split()) #split by whitespaces\n",
    "\n",
    "res = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "for i in expanded_data['text']:\n",
    "    t = \"\"\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            w = ps.stem(j)\n",
    "            t += w\n",
    "            t += \" \"\n",
    "    res.append(t)\n",
    "expanded_data['text'] = res\n",
    "expanded_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(expanded_data['text'], expanded_data['flare'],test_size=0.3)\n",
    "\n",
    "# X = the entire text data , Y = entire target or flares\n",
    "X = expanded_data['text']\n",
    "Y = expanded_data['flare']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# label encode the target variable \n",
    "Y = encoder.fit_transform(Y)\n",
    "train_y = encoder.transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n",
    "Tfidf_vect = TfidfVectorizer(max_features=15000)\n",
    "Tfidf_vect.fit(expanded_data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(train_x)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(test_x)\n",
    "X = Tfidf_vect.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  57.077401147858076\n"
     ]
    }
   ],
   "source": [
    "get_accuracy(Train_X_Tfidf,train_y,Test_X_Tfidf,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "print('Original dataset shape %s' % Counter(Y))\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_undersampled, Y_undersampled = rus.fit_resample(X, Y)\n",
    "Train_X_Tfidf_oversampled, Test_X_Tfidf_oversampled, train_y_oversampled, test_y_oversampled = model_selection.train_test_split(X_oversampled, Y_oversampled,test_size=0.3)\n",
    "print(Counter(Y_undersampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Below code will download the model, tfidf vectorizer and encoder to be directly used in Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open(\"model.pkl\",\"wb\"))\n",
    "pickle.dump(Tfidf_vect, open(\"tfidf.pickle\", \"wb\"))\n",
    "np.save('classes.npy', encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is used to test the endpoint by sending request from test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with open('test.txt', 'rb') as f:\n",
    "    r = requests.post('https://reddit-flare-detector.herokuapp.com/automated_testing', files={'test.txt': f})\n",
    "    a = r.json()\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
